# üåê 062 - 3D Mapping

> **Level 6.2 - Point Cloud Mapping | Rich 3D Environment Reconstruction**

---

## üìå Overview

The **3D Mapping Controller** converts RGB-D camera data into **rich 3D point clouds** for advanced perception tasks. Unlike traditional 2D LIDAR maps, this creates detailed 3D representations enabling object recognition, navigation planning, and scene understanding‚Äîyour robot now understands full 3D environments!

### ‚ú® Key Features

- üìπ RGB-D image fusion (color + depth)
- ‚òÅÔ∏è Real-time point cloud generation
- üé® Colored point cloud (XYZRGB format)
- üîÑ Pinhole camera model implementation
- üìä 3D visualization support
- üß≠ Robot odometry tracking
- üì° ROS2 PointCloud2 publishing
- üéÆ Manual + Auto spin modes
- üíæ Point cloud structure optimization

### üìÇ Files in This Directory

| File | Purpose |
|------|---------|
| `062_3d_mapping.py` | 3D mapping controller |
| `062_3d_map.wbt` | Webots world file |
| `062_3d_mapping_doc.md` | Detailed documentation |

---

## üöÄ Quick Start

### Step 1Ô∏è‚É£: Start Webots

```bash
webots ~/ros2_ws/src/ce_webots/worlds/062_3d_map.wbt
```

### Step 2Ô∏è‚É£: Run 3D Mapper

```bash
ros2 run ce_webots 062_3d_mapping
```

### Manual Control

```
‚å®Ô∏è  CONTROLS:
  W / ‚Üë - Forward
  S / ‚Üì - Backward
  A / ‚Üê - Turn Left
  D / ‚Üí - Turn Right
  M - Toggle Auto Spin
  Q / ESC - Quit

üéØ Watch as the robot rotates and the point cloud builds!
```

---

## üîß How It Works

### 1. Initialize RGB-D System

```python
# RGB camera
self.camera_rgb = self.robot.getDevice('camera_rgb')
self.camera_rgb.enable(self.timestep)

# Depth camera
self.camera_depth = self.robot.getDevice('camera_depth')
self.camera_depth.enable(self.timestep)

# Specifications
self.WIDTH = 128
self.HEIGHT = 128
self.FOV = 1.3  # radians
self.RANGE_MIN = 0.1  # meters
self.RANGE_MAX = 3.0  # meters
```

### 2. Camera Intrinsic Calculation

```python
def setup_camera_intrinsics(self):
    """Calculate focal length and principal point"""
    
    # Focal length from FOV
    f_x = f_y = self.WIDTH / (2 * math.tan(self.FOV / 2))
    
    # Principal point at image center
    c_x = self.WIDTH / 2
    c_y = self.HEIGHT / 2
    
    self.K = {
        'fx': f_x,
        'fy': f_y,
        'cx': c_x,
        'cy': c_y
    }
```

### 3. Point Cloud Generation

```python
def generate_pointcloud(self):
    """Convert RGB-D to colored 3D point cloud"""
    
    # Get RGB and depth images
    rgb_data = self.camera_rgb.getImage()
    depth_data = self.camera_depth.getRangeImage()
    
    # Convert to NumPy arrays
    rgb = np.frombuffer(rgb_data, np.uint8)
    rgb = rgb.reshape((self.HEIGHT, self.WIDTH, 4))[:, :, :3]  # BGRA‚ÜíBGR
    
    depth = np.array(depth_data)
    depth = depth.reshape((self.HEIGHT, self.WIDTH))
    
    # Filter valid depth range
    valid_mask = (depth >= self.RANGE_MIN) & (depth <= self.RANGE_MAX)
    
    # Create coordinate grids
    u, v = np.meshgrid(
        np.arange(self.WIDTH),
        np.arange(self.HEIGHT)
    )
    
    # Backproject to 3D using pinhole camera model
    X = (u - self.K['cx']) * depth / self.K['fx']
    Y = (v - self.K['cy']) * depth / self.K['fy']
    Z = depth
    
    # Extract RGB
    R = rgb[:, :, 2]  # Correct BGR‚ÜíRGB
    G = rgb[:, :, 1]
    B = rgb[:, :, 0]
    
    # Pack into PointCloud2 structure
    points_array = np.zeros(
        (self.HEIGHT * self.WIDTH,),
        dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'), ('rgb', 'f4')]
    )
    
    points_array['x'] = X.flatten()
    points_array['y'] = Y.flatten()
    points_array['z'] = Z.flatten()
    
    # Pack RGB into single float (for ROS2 compatibility)
    rgb_packed = (R.flatten() << 16) | (G.flatten() << 8) | B.flatten()
    points_array['rgb'] = rgb_packed.astype('f4')
    
    return points_array, valid_mask
```

### 4. Publish Point Cloud

```python
def publish_pointcloud(self, points):
    """Publish PointCloud2 message to ROS2"""
    
    # Create PointCloud2 message
    header = Header()
    header.stamp = self.get_clock().now().to_msg()
    header.frame_id = 'camera_rgb_optical_frame'
    
    # PointCloud2 fields
    fields = [
        PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
        PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
        PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
        PointField(name='rgb', offset=12, datatype=PointField.FLOAT32, count=1),
    ]
    
    cloud = PointCloud2(
        header=header,
        height=1,  # Unorganized
        width=len(points),
        fields=fields,
        is_bigendian=False,
        point_step=16,  # 4 √ó 4 bytes
        row_step=len(points) * 16,
        data=points.tobytes(),
        is_dense=True
    )
    
    self.pointcloud_publisher.publish(cloud)
```

---

## üìä Point Cloud Format

```
PointCloud2 Structure (Unorganized):
  
  Each point:  X (f4) | Y (f4) | Z (f4) | RGB (f4)
               4 bytes   4 bytes   4 bytes   4 bytes
               ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
               total: 16 bytes per point
  
  For 128√ó128 image:
    307,200 points √ó 16 bytes = 4.9 MB
```

---

## üîÑ Odometry Integration

```python
def update_robot_pose(self):
    """Track robot position for odometry"""
    
    # Read encoders
    left_pos = self.left_encoder.getValue()
    right_pos = self.right_encoder.getValue()
    
    # Calculate distance
    left_dist = (left_pos - self.prev_left) * self.WHEEL_RADIUS
    right_dist = (right_pos - self.prev_right) * self.WHEEL_RADIUS
    delta_s = (left_dist + right_dist) / 2
    
    # Update position
    self.x += delta_s * math.cos(self.theta)
    self.y += delta_s * math.sin(self.theta)
    
    # Publish odometry
    self.publish_odometry()
```

---

## üìö Sensor Knowledge: Point Cloud & 3D Mapping

### üéØ How It Works

**Point Cloud** is a collection of 3D points (X, Y, Z) representing environment surfaces:

```
Point Cloud Generation:

Step 1: Capture RGB-D Image
   RGB Image: (640√ó480) color per pixel
   Depth Map: (640√ó480) distance per pixel

Step 2: Backproject Each Pixel
   For pixel (x, y) with depth z:
   X = (x - cx) * z / fx
   Y = (y - cy) * z / fy
   Z = z
   
   Result: One 3D point (X, Y, Z)

Step 3: Accumulate Over Time
   Frame 1: 307,200 points
   Frame 2: 307,200 points + robot moved
   Frame 3: 307,200 points + robot moved
   ...
   
   Total: Millions of points = 3D map!

Point Cloud Structure:
  points_cloud[i] = (x, y, z, r, g, b)
  
  Example:
  ‚îú‚îÄ Point 0: (-0.5,  0.2, 1.0, 255, 0, 0)  # Red
  ‚îú‚îÄ Point 1: (-0.4,  0.3, 1.1, 255, 0, 0)  # Red
  ‚îú‚îÄ Point 2: ( 0.0,  0.0, 0.8,   0, 255, 0)  # Green
  ‚îî‚îÄ Point N: ( 5.0,  3.0, 2.5,   0, 0, 255)  # Blue
```

### üìä Specifications

| ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ | ‡∏Ñ‡πà‡∏≤ | ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏ |
|---------|-----|--------|
| **Points per Frame** | 307,200 | 640√ó480 RGB-D |
| **Capture Rate** | ~30 Hz | 33ms per frame |
| **Accumulation** | Real-time | As robot moves |
| **Max Points** | Millions | Depends on memory |
| **Color per Point** | RGB 8-bit | (R, G, B) 0-255 |
| **Coordinates** | 32-bit float | (X, Y, Z) meters |
| **Memory per Point** | ~16 bytes | XYZ (12) + RGB (3) + padding |

### üí° Usage Tips

**‚úÖ ‡∏ó‡∏≥‡πÑ‡∏î‡πâ:**
- Complete 3D environment mapping
- Object localization in 3D space
- Voxel grid representation
- Surface reconstruction
- Obstacle avoidance planning

**‚ùå ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á:**
- Keep ALL points (memory explosion)
- No filtering/downsampling
- Processing without GPU
- Ignoring coordinate transformation
- Not handling outliers

### ‚ö†Ô∏è Limitations

```
1. Memory Explosion
   
   Point Count Growth:
   ‚îú‚îÄ 1 frame: 307K points = ~5 MB
   ‚îú‚îÄ 10 frames: 3.07M points = ~50 MB
   ‚îú‚îÄ 100 frames: 30.7M points = ~500 MB
   ‚îú‚îÄ 1000 frames: 307M points = ~5 GB ‚ö†Ô∏è
   
   Solutions:
   ‚îú‚îÄ Voxel grid decimation
   ‚îú‚îÄ Spatial downsampling
   ‚îú‚îÄ Sliding window (keep recent)
   ‚îî‚îÄ GPU point cloud processing

2. Computational Load
   ‚îú‚îÄ ICP (Iterative Closest Point) is slow
   ‚îú‚îÄ 100M points = seconds to process
   ‚îî‚îÄ‚Üí Use hierarchical approaches

3. Outliers & Noise
   ‚îú‚îÄ Sensor noise = spurious points
   ‚îú‚îÄ Dynamic objects = transients
   ‚îú‚îÄ Shadows = empty regions
   ‚îî‚îÄ‚Üí Statistical outlier removal

4. Coordinate Transformations
   ‚îú‚îÄ Each frame: different camera pose
   ‚îú‚îÄ Must transform to world coordinates
   ‚îú‚îÄ Errors accumulate
   ‚îî‚îÄ‚Üí Use SLAM for correction

5. Occlusions
   ‚îú‚îÄ Blocked views = missing regions
   ‚îú‚îÄ Can't see inside objects
   ‚îî‚îÄ‚Üí Multiple viewpoints needed
```

### üîß Point Cloud Processing

```python
import numpy as np
from scipy.spatial import KDTree

class PointCloud3D:
    
    def __init__(self, points_xyz, colors_rgb=None):
        """Initialize point cloud
        
        Args:
            points_xyz: (N, 3) array of 3D coordinates
            colors_rgb: (N, 3) array of RGB values (0-255)
        """
        self.points = np.array(points_xyz, dtype=np.float32)
        self.colors = np.array(colors_rgb, dtype=np.uint8) if colors_rgb is not None else None
        self._kdtree = None
    
    def build_kdtree(self):
        """Build KD-tree for fast nearest neighbor search"""
        self._kdtree = KDTree(self.points)
    
    def remove_outliers(self, k=10, std_ratio=1.0):
        """Statistical outlier removal (covariance analysis)"""
        
        if self._kdtree is None:
            self.build_kdtree()
        
        # Find k nearest neighbors for each point
        distances, indices = self._kdtree.query(
            self.points, k=k+1
        )
        
        # Compute mean and std of distances
        mean_dist = np.mean(distances, axis=1)
        std_dist = np.std(distances, axis=1)
        threshold = np.mean(mean_dist) + std_ratio * np.std(mean_dist)
        
        # Keep points below threshold
        mask = mean_dist < threshold
        
        self.points = self.points[mask]
        if self.colors is not None:
            self.colors = self.colors[mask]
        
        self._kdtree = None  # Rebuild needed
        
        return np.sum(mask)  # Points removed
    
    def downsample_voxel(self, voxel_size=0.01):
        """Voxel grid downsampling"""
        
        # Compute voxel indices
        voxel_indices = np.floor(
            self.points / voxel_size
        ).astype(np.int32)
        
        # Create lookup table
        voxel_dict = {}
        for i, idx in enumerate(voxel_indices):
            key = tuple(idx)
            if key not in voxel_dict:
                voxel_dict[key] = []
            voxel_dict[key].append(i)
        
        # Keep one point per voxel (centroid)
        new_points = []
        new_colors = []
        
        for indices_in_voxel in voxel_dict.values():
            # Compute voxel centroid
            centroid = np.mean(
                self.points[indices_in_voxel], axis=0
            )
            new_points.append(centroid)
            
            # Use color from first point
            if self.colors is not None:
                new_colors.append(self.colors[indices_in_voxel[0]])
        
        self.points = np.array(new_points, dtype=np.float32)
        if self.colors is not None:
            self.colors = np.array(new_colors, dtype=np.uint8)
        
        self._kdtree = None
        
        return len(new_points)  # Remaining points

    def transform(self, rotation_matrix, translation_vector):
        """Apply rigid transformation (rotation + translation)"""
        
        # (X', Y', Z') = R * (X, Y, Z) + T
        self.points = (rotation_matrix @ self.points.T).T + translation_vector

    def estimate_normals(self, k=10):
        """Estimate surface normals (tangent plane)"""
        
        if self._kdtree is None:
            self.build_kdtree()
        
        # Find k nearest neighbors
        distances, indices = self._kdtree.query(
            self.points, k=k
        )
        
        normals = []
        
        for i, neighbor_indices in enumerate(indices):
            # Get neighbor points
            neighbors = self.points[neighbor_indices]
            
            # Compute covariance matrix
            centered = neighbors - neighbors[0]
            cov = centered.T @ centered
            
            # Smallest eigenvector = normal
            eigenvalues, eigenvectors = np.linalg.eigh(cov)
            normal = eigenvectors[:, 0]  # Smallest eigenvalue
            
            normals.append(normal)
        
        return np.array(normals)
    
    def crop(self, x_range, y_range, z_range):
        """Crop point cloud to region of interest"""
        
        mask = (
            (self.points[:, 0] >= x_range[0]) & 
            (self.points[:, 0] <= x_range[1]) &
            (self.points[:, 1] >= y_range[0]) & 
            (self.points[:, 1] <= y_range[1]) &
            (self.points[:, 2] >= z_range[0]) & 
            (self.points[:, 2] <= z_range[1])
        )
        
        self.points = self.points[mask]
        if self.colors is not None:
            self.colors = self.colors[mask]
        
        return np.sum(mask)  # Remaining points
```

### üìä 3D Mapping Pipeline

```
RGB-D Stream
    ‚Üì
Estimate Camera Pose (SLAM)
    ‚Üì
Transform Points to World Frame
    ‚Üì
Accumulate Points (global cloud)
    ‚Üì
Downsample (voxel grid)
    ‚Üì
Remove Outliers
    ‚Üì
Loop Closure Detection
    ‚Üì
Map Optimization
    ‚Üì
Surface Reconstruction (Poisson)
    ‚Üì
Export (PLY/PCD format)
    ‚Üì
Visualization/Navigation
```

### üìä Voxel Grid Representation

```
Alternative: Occupancy Voxel Grid

Traditional Point Cloud:
‚îî‚îÄ Millions of points
   ‚îú‚îÄ High precision
   ‚îú‚îÄ High memory
   ‚îî‚îÄ Slow processing

Voxel Grid (3D pixels):
‚îú‚îÄ Fixed grid cells
‚îú‚îÄ Each cell: occupied/free
‚îú‚îÄ Fast ray casting
‚îú‚îÄ Memory efficient
‚îî‚îÄ Example: 10√ó10√ó10 @ 0.1m = complete 1m¬≥ room

Voxel States:
  0: Unknown (not observed)
  1: Free (observed empty)
  2: Occupied (observed obstacle)

Benefits:
‚úì Fast collision checking
‚úì Easy navigation planning
‚úì Bounded memory
‚úì GPU acceleration
```

---

## üéì Learning Outcomes

After using this controller, you'll understand:

- ‚úÖ Point cloud generation from RGB-D
- ‚úÖ Pinhole camera model application
- ‚úÖ Color-depth image fusion
- ‚úÖ PointCloud2 message format
- ‚úÖ Efficient point packing
- ‚úÖ Odometry integration
- ‚úÖ ROS2 TF broadcasting
- ‚úÖ 3D visualization in RViz2

---

## üìê Pinhole Camera Model Math

```
2D Image Coordinates (u, v) ‚Üí 3D World (X, Y, Z)

Camera Frame (at focal point):
  X = (u - cx) √ó Z / fx
  Y = (v - cy) √ó Z / fy
  Z = depth (from RangeFinder)

Where:
  fx, fy = Focal lengths (pixels)
  cx, cy = Principal point (pixels)
  u, v = Image coordinates (pixels)
  Z = Depth value (meters)
```

---

## üìù Customization

### Lower Resolution (Faster)

```python
self.WIDTH = 64
self.HEIGHT = 64
# Results in 16√ó fewer points
```

### Increase Range

```python
self.RANGE_MAX = 5.0  # Detect further objects
```

### Auto Spin Configuration

```python
AUTO_SPIN_SPEED = 2.0  # rad/s
AUTO_SPIN_PERIOD = 6.28  # Full rotation (~6 seconds)
```

### Filter by Color

```python
# Detect only red objects
red_mask = (rgb[:,:,2] > 150) & (rgb[:,:,0] < 50)
valid_mask &= red_mask
```

---

## üìö Related Resources

- üìñ [Point Cloud Library](https://pointclouds.org/)
- üîó [Pinhole Camera](https://en.wikipedia.org/wiki/Pinhole_camera_model)
- üìê [3D Geometry](https://en.wikipedia.org/wiki/3D_computer_graphics)
- ü§ñ [ROS 2 PointCloud2](https://docs.ros2.org/latest/api/sensor_msgs/msg/PointCloud2.html)
- üì∫ [RViz2 Point Cloud Visualization](https://github.com/ros2/rviz)
- üëÄ [061 Depth Camera](../061_depth_camera_controller/)
- üëÄ [052 SLAM](../052_slam_controller/)

---

## ‚ö†Ô∏è Troubleshooting

| Issue | Solution |
|-------|----------|
| **No point cloud** | Verify cameras enabled; check publisher connection |
| **Distorted cloud** | Recalibrate intrinsics; check FOV setting |
| **Performance slow** | Reduce resolution; downsample points |
| **Memory issues** | Process fewer points; stream instead of store |

---

## **üë§ Authors**

- üöÄ [@alfaXphoori](https://www.github.com/alfaXphoori)

---

<div align="center">

**Made with ‚ù§Ô∏è for the ROS 2 Community**

</div>
