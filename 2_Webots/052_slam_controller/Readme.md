# üó∫Ô∏è 052 - SLAM Controller

> **Level 5.2 - Simultaneous Localization and Mapping | Autonomous Navigation Without GPS**

---

## üìå Overview

Level 5.2: SLAM with Odometry (No GPS)
======================================
Creating maps without GPS, using wheel-based position calculation (Odometry)

Features:
1. Odometry: Calculate X, Y, Theta coordinates from Encoder and IMU
2. Lidar Mapping: Draw walls onto the grid (Occupancy Grid)
3. Console Visualizer: Display live map on screen
4. RViz Marker: Visualizes the robot body

Author: AI Assistant

### ‚ú® Key Features

- üìç Odometry-based localization (encoder + IMU)
- üó∫Ô∏è Real-time occupancy grid mapping
- üîÑ LIDAR-based obstacle mapping
- üß≠ Autonomous intelligent navigation
- üìä Wall-following and exploration behaviors
- üéØ Stuck detection and recovery
- üì° ROS2 integration (LaserScan, OccupancyGrid, Odometry, TF)
- üé® Real-time dashboard display
- üåç 10m √ó 10m coverage area

### üìÇ Files in This Directory

| File | Purpose |
|------|---------|
| `052_slam_controller.py` | SLAM navigation controller |
| `052_slam.wbt` | Webots world file |


---

## üöÄ How to Run This Lab

### Prerequisites

- ‚úÖ Webots installed
- ‚úÖ ROS 2 Jazzy installed and sourced
- ‚úÖ **RViz2 installed** (`sudo apt install ros-jazzy-rviz2`)
- ‚úÖ Understanding of Lab 051 (LIDAR basics)
- ‚úÖ Workspace built and sourced

### Running Steps

#### Terminal 1: Launch Webots Simulation

```bash
webots ~/ros2_ws/src/ce_webots/worlds/052_slam.wbt
```

**Environment:**
- Large arena with multiple rooms and corridors
- Various obstacles for mapping
- Starting position in open area
- Up to 10m √ó 10m coverage area

#### Terminal 2: Run SLAM Controller

```bash
# Source your workspace
source ~/ros2_ws/install/setup.bash

# Run the SLAM controller
ros2 run ce_webots 052_slam_controller
```

**What to observe:**
- Real-time position tracking (X, Y, Œ∏)
- Occupancy grid map building
- Autonomous exploration behavior
- Map coverage statistics
- Terminal-based map visualization

#### Terminal 3: Launch RViz2 for Visualization

```bash
# Source ROS 2
source /opt/ros/jazzy/setup.bash
source ~/ros2_ws/install/setup.bash

# Launch RViz2
rviz2
```

### üìä RViz Configuration for SLAM

Follow these steps to visualize the SLAM data in RViz:

#### Step 1: Set Fixed Frame

In the left panel under "Global Options":
- Set **Fixed Frame** to `odom`

#### Step 2: Add LaserScan Display

1. Click **Add** button (bottom left)
2. Select **By display type** tab
3. Choose **LaserScan**
4. Click **OK**

Configure LaserScan:
- **Topic:** `/scan`
- **Size (m):** `0.05`
- **Color:** Red or Yellow
- **Decay Time:** `0`

#### Step 3: Add Map Display

1. Click **Add** button
2. Select **Map**
3. Click **OK**

Configure Map:
- **Topic:** `/map`
- **Color Scheme:** map or costmap
- **Alpha:** `0.7`

#### Step 4: Add Odometry Display

1. Click **Add** button
2. Select **Odometry**
3. Click **OK**

Configure Odometry:
- **Topic:** `/odom`
- **Position Tolerance:** `0.1`
- **Angle Tolerance:** `0.1`
- **Keep:** `100` (show trail)

Optional - Add Arrow for heading:
- **Shape:** Arrow
- **Color:** Blue
- **Arrow Length:** `0.3`

#### Step 5: Add RobotModel (Optional)

1. Click **Add** button
2. Select **RobotModel**
3. Click **OK**

This shows a 3D model of the robot if URDF is available.

#### Step 6: Add Marker for Robot Body

1. Click **Add** button  
2. Select **Marker**
3. Click **OK**

Configure Marker:
- **Topic:** `/robot_marker`

### Save RViz Configuration

To save your setup:
```bash
File ‚Üí Save Config As ‚Üí ~/ros2_ws/src/ce_webots/config/052_slam.rviz
```

Next time, load directly:
```bash
rviz2 -d ~/ros2_ws/src/ce_webots/config/052_slam.rviz
```

### What You Should See in RViz

- **Red/Yellow dots:** LIDAR scan points (real-time obstacles)
- **Gray occupancy grid:** The map being built
- **Black cells:** Detected obstacles/walls
- **White cells:** Free space
- **Gray cells:** Unknown/unexplored areas
- **Blue arrow:** Robot position and heading
- **Trail:** Robot's path history

### Interactive Controls (Terminal 2)

- **M** - Toggle between MANUAL and AUTO exploration modes
- **R** - Reset map (start mapping over)
- **Q** - Quit program

### Real-Time SLAM Dashboard

```
================================================================================
              ü§ñ SLAM ODOMETRY & MAPPING CONTROLLER üó∫Ô∏è
================================================================================

üìç POSITION & ORIENTATION
   X:     1.245 m    [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ]
   Y:     2.376 m    [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ]
   Œ∏:    45.32 ¬∞    [‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ]

üó∫Ô∏è MAP STATUS (200√ó200 cells, 5cm resolution)
   Mapped Area:   1540 cells
   Obstacles:     340 cells
   Unknown:       2120 cells
   [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 41% Coverage

üì° LIDAR SCAN (360 points)
   Front:    0.85m üü¢
   Left:     0.42m üü°
   Right:    0.68m üü¢
   Back:     1.20m üü¢

üß≠ NAVIGATION STATE
   Mode:     AUTONOMOUS EXPLORATION
   Status:   ‚û°Ô∏è Moving forward
   Heading:  Northeast ‚ÜóÔ∏è

‚å®Ô∏è  CONTROLS: M=Mode Toggle | R=Reset Map | Q=Quit
================================================================================
```

### Monitoring Topics

```bash
# Terminal 4 (optional): Check published topics
ros2 topic list

# Monitor map updates
ros2 topic hz /map

# View occupancy grid data
ros2 topic echo /map --once

# Monitor odometry
ros2 topic echo /odom

# Check TF transforms
ros2 run tf2_ros tf2_echo odom base_link
```

### Understanding SLAM Data

**Published Topics:**
- `/scan` - LaserScan data from LIDAR
- `/map` - OccupancyGrid (the built map)
- `/odom` - Odometry (robot position)
- `/tf` - Transform tree (coordinate frames)
- `/robot_marker` - Visualization marker

**Map Resolution:**
- Grid: 200√ó200 cells
- Cell size: 5cm (0.05m)
- Total coverage: 10m √ó 10m
- Unknown cells: -1
- Free cells: 0
- Occupied cells: 100

### Troubleshooting

| Issue | Solution |
|-------|----------|
| **RViz shows nothing** | Check Fixed Frame is set to `odom` |
| **No LaserScan visible** | Verify topic `/scan` is being published |
| **Map not building** | Check LIDAR data and robot is moving |
| **Robot position wrong** | Verify encoders and IMU are working |
| **RViz crashes** | Reduce Decay Time or Keep value in displays |
| **TF errors** | Check that TF tree is being published correctly |

### Performance Tips

1. **Map Quality:**
   - Move slowly for better accuracy
   - Revisit areas for loop closure
   - Avoid rapid rotations

2.**RViz Performance:**
   - Reduce LaserScan point size
   - Lower Decay Time
   - Limit Keep history

3. **Exploration:**
   - Let robot explore systematically
   - Cover all rooms and corridors
   - Check map coverage percentage

---

## üîß How It Works

### 1. Initialize Sensors

```python
# Wheel encoders
self.left_encoder = self.robot.getDevice('left_wheel_sensor')
self.right_encoder = self.robot.getDevice('right_wheel_sensor')

# IMU for heading
self.imu = self.robot.getDevice('inertial_unit')

# LIDAR for mapping
self.lidar = self.robot.getDevice('lidar')

# Enable all sensors
for sensor in [self.left_encoder, self.right_encoder, self.imu, self.lidar]:
    sensor.enable(self.timestep)

# Odometry tracking
self.x = 0.0
self.y = 0.0
self.theta = 1.57  # Start facing North (90¬∞)
```

### 2. Odometry Calculation (Encoder + IMU)

```python
def update_odometry(self):
    """Update position using encoders and IMU"""
    
    # Read wheel encoders
    left_pos = self.left_encoder.getValue()
    right_pos = self.right_encoder.getValue()
    
    # Calculate distances
    left_dist = (left_pos - self.prev_left) * self.WHEEL_RADIUS
    right_dist = (right_pos - self.prev_right) * self.WHEEL_RADIUS
    
    # Differential drive kinematics
    delta_s = (left_dist + right_dist) / 2.0  # Distance traveled
    delta_theta = (right_dist - left_dist) / self.AXLE_LENGTH  # Rotation
    
    # Get heading from IMU (more accurate than encoders)
    quat = self.imu.getQuaternion()
    self.theta = self.quaternion_to_yaw(quat)
    
    # Update position
    if delta_s != 0:
        self.x += delta_s * math.cos(self.theta)
        self.y += delta_s * math.sin(self.theta)
    
    # Remember for next iteration
    self.prev_left = left_pos
    self.prev_right = right_pos
```

### 3. Occupancy Grid Mapping

```python
def update_map(self, lidar_distances):
    """Update occupancy grid from LIDAR scan"""
    
    for angle in range(360):
        distance = lidar_distances[angle]
        
        if distance < self.MAX_RANGE:
            # Convert polar (angle, distance) to world coordinates
            world_x = self.x + distance * math.cos(self.theta + math.radians(angle))
            world_y = self.y + distance * math.sin(self.theta + math.radians(angle))
            
            # Convert to grid cell
            cell_x = int((world_x + self.MAP_OFFSET) / self.RESOLUTION)
            cell_y = int((world_y + self.MAP_OFFSET) / self.RESOLUTION)
            
            # Update occupancy grid
            if 0 <= cell_x < self.MAP_SIZE and 0 <= cell_y < self.MAP_SIZE:
                self.occupancy_grid[cell_y][cell_x] = 100  # Occupied
```

### 4. Autonomous Navigation

```python
def navigate(self, lidar_distances):
    """Autonomous exploration with obstacle avoidance"""
    
    sectors = self.analyze_lidar(lidar_distances)
    
    # Check for stuck condition
    if self.detect_stuck():
        return self.recover_from_stuck()
    
    # Multi-level obstacle avoidance
    if sectors['front'] < 0.2:
        return self.EMERGENCY_REVERSE()
    elif sectors['front'] < 0.3:
        if sectors['left'] > sectors['right']:
            return self.TURN_LEFT()
        else:
            return self.TURN_RIGHT()
    elif sectors['left'] < 0.25 or sectors['right'] < 0.25:
        return self.ADJUST_SIDES()
    else:
        return self.MOVE_FORWARD()
```

### 5. Stuck Detection & Recovery

```python
def detect_stuck(self):
    """Check if robot is not making progress"""
    # Store position every 50 timesteps
    if self.timestep_counter % 50 == 0:
        distance_moved = math.sqrt(
            (self.x - self.last_check_x)**2 +
            (self.y - self.last_check_y)**2
        )
        
        # Stuck if moved < 0.05m in 50 steps
        if distance_moved < 0.05:
            return True
    
    return False

def recover_from_stuck(self):
    """Escape from stuck situations"""
    if self.recovery_state == BACKING_UP:
        self.move_backward()
    elif self.recovery_state == TURNING:
        self.turn_left()  # Or right
    elif self.recovery_state == DONE:
        self.recovery_state = BACKING_UP
```

---

## üó∫Ô∏è Map Representation

```
Occupancy Grid (200√ó200 cells):
  Each cell = 5cm √ó 5cm
  Total coverage = 10m √ó 10m
  
  Cell values:
    0 = Unknown
    50 = Free space
    100 = Occupied (obstacle)
    
Coordinate System:
  (-5m, +5m) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (+5m, +5m)
      ‚îÇ                               ‚îÇ
      ‚îÇ        Robot Path             ‚îÇ
      ‚îÇ      ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤          ‚îÇ
      ‚îÇ     ‚ï±       ü§ñ      ‚ï≤         ‚îÇ
      ‚îÇ    ‚îÇ   (1.2, 2.4)    ‚îÇ        ‚îÇ
      ‚îÇ    ‚îÇ                 ‚îÇ        ‚îÇ
      ‚îÇ     ‚ï≤               ‚ï±         ‚îÇ
      ‚îÇ      ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï±          ‚îÇ
      ‚îÇ                               ‚îÇ
  (-5m, -5m) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (+5m, -5m)
```

---

## üìö Sensor Knowledge: Odometry + LIDAR Fusion

### üéØ How It Works

**Odometry** calculates position from wheel movement, **LIDAR** provides distance data to obstacles:

```
Sensor Fusion Architecture:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Encoders  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Odometry ‚îÇ
‚îÇ  (Wheels)  ‚îÇ         ‚îÇ(X, Y, Œ∏) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                      SLAM Algorithm
                             ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ   LIDAR    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îú‚îÄ‚Üí Map
‚îÇ(360¬∞ scan) ‚îÇ               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îú‚îÄ‚Üí Position
                       ‚îÇ Correction
                       ‚îÇ
                    Particle Filter
                   (Adaptive Monte Carlo)

Timeline:
  t=0:    Robot at (0, 0, 0¬∞)
  t=1:    Move forward ‚Üí Odometry: (0.1, 0, 0¬∞)
  t=2:    Detect wall at 0.5m front
  t=3:    Update map; correct position
  t=‚àû:    Create complete map + accurate pose
```

### üìä Specifications

| Property | Value | Notes |
|---------|-------|-------|
| **Odometry Source** | Encoders + IMU | Proprioceptive |
| **LIDAR Rays** | 360 | One per degree |
| **LIDAR Range** | 0.05-2.0 m | Max detection |
| **Map Resolution** | 0.01 m (1cm) | Grid cell size |
| **Update Rate** | ~10 Hz | SLAM updates |
| **Max Drift** | ~5% per meter | Without corrections |
| **Correction Freq** | Variable | When landmarks match |

### üí° Usage Tips

**‚úÖ Do:**
- Loop closure detection
- Multi-robot SLAM
- Real-time mapping
- Online optimization
- Map saving/loading

**‚ùå Avoid:**
- Ignoring odometry drift
- Processing LIDAR too fast
- No loop closure check
- Using only odometry for localization
- Not filtering sensor noise

### ‚ö†Ô∏è Limitations

```
1. Odometry Drift
   
   Cumulative Error Over Time:
   ‚îú‚îÄ After 10m: ¬±0.5-1.0m error
   ‚îú‚îÄ After 50m: ¬±2-5m error
   ‚îî‚îÄ After 100m: ¬±10-20m error
   
   Causes:
   ‚îú‚îÄ Wheel slippage
   ‚îú‚îÄ Encoder resolution
   ‚îú‚îÄ Surface friction changes
   ‚îî‚îÄ‚Üí Solution: LIDAR loop closure!

2. LIDAR Reflections
   ‚îú‚îÄ Glass/mirrors: no detection
   ‚îú‚îÄ Dark objects: poor detection
   ‚îú‚îÄ Reflective surfaces: false positives
   ‚îî‚îÄ‚Üí Use multiple scans to confirm

3. Loop Closure Challenges
   ‚îú‚îÄ Must recognize same place
   ‚îú‚îÄ Needs distinct features
   ‚îú‚îÄ Kidnapped robot problem
   ‚îî‚îÄ‚Üí Machine learning can help

4. Computational Load
   ‚îú‚îÄ 360 rays √ó 10 Hz = processing
   ‚îú‚îÄ SLAM algorithms are CPU-intensive
   ‚îî‚îÄ‚Üí Solution: Downsample/optimization

5. Dynamic Environments
   ‚îú‚îÄ Moving people = ghosts in map
   ‚îú‚îÄ Doors opening/closing
   ‚îî‚îÄ‚Üí Solution: Temporal filtering
```

### üîß Odometry Calculation

```python
import numpy as np
from math import cos, sin

def calculate_odometry(left_encoder, right_encoder, 
                      left_prev, right_prev,
                      wheel_radius, wheel_distance):
    """Calculate position change from encoder deltas"""
    
    # 1. Calculate wheel rotations
    left_delta = left_encoder - left_prev
    right_delta = right_encoder - right_prev
    
    # 2. Convert to linear distances
    left_distance = left_delta * wheel_radius
    right_distance = right_delta * wheel_radius
    
    # 3. Calculate average distance (forward)
    forward = (left_distance + right_distance) / 2.0
    
    # 4. Calculate rotation
    turning = (right_distance - left_distance) / wheel_distance
    
    return forward, turning

def update_pose(pose_x, pose_y, pose_theta,
               forward, turning):
    """Update robot pose based on motion"""
    
    # Simple motion model (differential drive)
    
    if abs(turning) < 0.001:  # Straight line
        # No rotation
        new_x = pose_x + forward * cos(pose_theta)
        new_y = pose_y + forward * sin(pose_theta)
        new_theta = pose_theta
    
    else:  # Curved path
        # Calculate radius of curvature
        radius = forward / turning
        
        # Integrate arc motion
        new_theta = pose_theta + turning
        
        # Forward kinematics
        new_x = pose_x + radius * (sin(new_theta) - sin(pose_theta))
        new_y = pose_y - radius * (cos(new_theta) - cos(pose_theta))
    
    return new_x, new_y, new_theta

def grid_map_update(grid_map, pose_x, pose_y, 
                   lidar_ranges, resolution=0.01):
    """Update occupancy grid with LIDAR data"""
    
    # Convert to grid indices
    robot_grid_x = int(pose_x / resolution)
    robot_grid_y = int(pose_y / resolution)
    
    # Process each LIDAR ray
    for angle_deg in range(360):
        angle_rad = np.radians(angle_deg)
        distance = lidar_ranges[angle_deg]
        
        if distance > 2.0:
            continue  # Skip invalid readings
        
        # Calculate hit position
        hit_x = pose_x + distance * cos(angle_rad)
        hit_y = pose_y + distance * sin(angle_rad)
        
        # Convert to grid
        hit_grid_x = int(hit_x / resolution)
        hit_grid_y = int(hit_y / resolution)
        
        # Mark as occupied
        grid_map[hit_grid_x, hit_grid_y] = 255  # Occupied
        
        # Bresenham line: mark as free between robot and hit
        line_cells = bresenham_line(
            robot_grid_x, robot_grid_y,
            hit_grid_x, hit_grid_y
        )
        for cell in line_cells[:-1]:  # Exclude endpoint
            grid_map[cell[0], cell[1]] = 0  # Free

def loop_closure_detection(current_scan, map_scans, 
                          threshold=0.8):
    """Detect when robot revisits previously mapped area"""
    
    best_match_idx = -1
    best_correlation = 0
    
    # Compare current scan with all previous scans
    for idx, map_scan in enumerate(map_scans):
        # Simple correlation metric
        correlation = np.corrcoef(
            current_scan, map_scan
        )[0, 1]
        
        if correlation > best_correlation:
            best_correlation = correlation
            best_match_idx = idx
    
    # Loop closure detected if correlation high enough
    if best_correlation > threshold:
        return True, best_match_idx, best_correlation
    else:
        return False, None, best_correlation

# Bresenham line algorithm (for raycasting)
def bresenham_line(x0, y0, x1, y1):
    """Get grid cells between two points"""
    points = []
    dx = abs(x1 - x0)
    dy = abs(y1 - y0)
    sx = 1 if x0 < x1 else -1
    sy = 1 if y0 < y1 else -1
    err = dx - dy
    
    x, y = x0, y0
    while True:
        points.append((x, y))
        if x == x1 and y == y1:
            break
        e2 = 2 * err
        if e2 > -dy:
            err -= dy
            x += sx
        if e2 < dx:
            err += dx
            y += sy
    
    return points
```

### üìä SLAM Algorithm Flow

```
1. MOTION MODEL
   ‚îú‚îÄ Read encoders
   ‚îú‚îÄ Calculate Œîx, Œîy, ŒîŒ∏
   ‚îî‚îÄ‚Üí Predict new pose

2. MEASUREMENT UPDATE
   ‚îú‚îÄ Read LIDAR scan
   ‚îú‚îÄ Match to map
   ‚îî‚îÄ‚Üí Correct pose estimate

3. MAP UPDATE
   ‚îú‚îÄ Add scan to map
   ‚îú‚îÄ Update occupancy grid
   ‚îî‚îÄ‚Üí Grow map

4. LOOP CLOSURE
   ‚îú‚îÄ New scan matches old scan?
   ‚îú‚îÄ Calculate correction
   ‚îî‚îÄ‚Üí Optimize entire trajectory

5. REPEAT
   ‚îî‚îÄ‚Üí Better map + accurate position

Benefits:
‚úì Drift correction
‚úì Complete map building
‚úì Self-correcting localization
```

---

## üéì Learning Outcomes

After using this controller, you'll understand:

- ‚úÖ Odometry calculation from encoders
- ‚úÖ IMU integration for accuracy
- ‚úÖ Occupancy grid concepts
- ‚úÖ Polar to Cartesian conversion
- ‚úÖ SLAM fundamentals
- ‚úÖ Stuck detection and recovery
- ‚úÖ ROS2 transform (TF) tree
- ‚úÖ Multi-sensor fusion

---

## üìä Performance Metrics

```
Odometry Accuracy:
  ‚úÖ Position Error: < 2% of distance
  ‚úÖ Heading Error: < 1¬∞ per 360¬∞ turn
  ‚úÖ Map Coverage: Up to 90% with careful navigation

Computational Requirements:
  ‚ö° Processing: ~10-20ms per cycle
  üíæ Memory: ~1.2 MB for map
  üìä Publish Rate: 10 Hz
```

---

## üìù Customization

### Adjust Map Parameters

```python
MAP_SIZE = 400           # Larger map (20m √ó 20m)
RESOLUTION = 0.025      # Finer resolution (2.5cm cells)
```

### Modify Navigation Aggressiveness

```python
EMERGENCY_DISTANCE = 0.25  # More aggressive
CRITICAL_DISTANCE = 0.4
SAFE_DISTANCE = 0.7
```

### Implement Loop Closure

```python
def detect_loop_closure(self):
    """Check if robot returned to known area"""
    # Find nearest previous position
    for prev_pos in self.position_history:
        dist = math.sqrt((self.x - prev_pos['x'])**2 + 
                        (self.y - prev_pos['y'])**2)
        if dist < 0.2:  # Returned to same spot
            return True
    return False
```

---

## üìö Related Resources

- üìñ [SLAM Fundamentals](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)
- üó∫Ô∏è [Occupancy Grids](https://en.wikipedia.org/wiki/Occupancy_grid_mapping)
- üìç [Odometry](https://en.wikipedia.org/wiki/Odometry)
- ü§ñ [ROS 2 SLAM Guide](https://docs.ros.org/en/nav2_tutorials/)
- üëÄ [051 LIDAR Controller](../051_lidar_controller/)
- üëÄ [071 Go to Goal](../071_go_to_goal/) (Advanced navigation)

---

## ‚ö†Ô∏è Troubleshooting

| Issue | Solution |
|-------|----------|
| **Map not updating** | Verify LIDAR enabled; check odometry calculation |
| **Position drifts** | Calibrate wheel radius; enable IMU heading |
| **Stuck detection fails** | Adjust stuck threshold; verify motor control |
| **TF errors** | Ensure clock synchronized; check frame names |

---

## **üë§ Authors**

- üöÄ [@alfaXphoori](https://www.github.com/alfaXphoori)

---

<div align="center">

**Made with ‚ù§Ô∏è for the ROS 2 Community**

</div>
